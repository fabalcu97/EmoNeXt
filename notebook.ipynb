{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ema-pytorch in ./.venv/lib/python3.11/site-packages (0.7.7)\n",
      "Requirement already satisfied: torch>=2.0 in ./.venv/lib/python3.11/site-packages (from ema-pytorch) (2.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=2.0->ema-pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch>=2.0->ema-pytorch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=2.0->ema-pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0->ema-pytorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=2.0->ema-pytorch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.0->ema-pytorch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0->ema-pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.0->ema-pytorch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ema-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-18T01:40:00.469797Z",
     "iopub.status.busy": "2025-02-18T01:40:00.469264Z",
     "iopub.status.idle": "2025-02-18T01:40:00.480438Z",
     "shell.execute_reply": "2025-02-18T01:40:00.479288Z",
     "shell.execute_reply.started": "2025-02-18T01:40:00.469749Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ema_pytorch import EMA\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torchvision.ops import StochasticDepth\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "# wandb.login(key=wandb_api_key)\n",
    "\n",
    "# seed = 2001\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:40:04.124346Z",
     "iopub.status.busy": "2025-02-18T01:40:04.124020Z",
     "iopub.status.idle": "2025-02-18T01:40:04.160435Z",
     "shell.execute_reply": "2025-02-18T01:40:04.158708Z",
     "shell.execute_reply.started": "2025-02-18T01:40:04.124322Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class DotProductSelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DotProductSelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        scale = 1 / math.sqrt(math.sqrt(self.input_dim))\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) * scale\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "        output = attended_values + x\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(\n",
    "                x, self.normalized_shape, self.weight, self.bias, self.eps\n",
    "            )\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    r\"\"\"ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, drop_path=0.0, layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(\n",
    "            dim, dim, kernel_size=7, padding=3, groups=dim\n",
    "        )  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(\n",
    "            dim, 4 * dim\n",
    "        )  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = (\n",
    "            nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n",
    "            if layer_scale_init_value > 0\n",
    "            else None\n",
    "        )\n",
    "        self.stochastic_depth = StochasticDepth(drop_path, \"row\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.stochastic_depth(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmoNeXt(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        depths=None,\n",
    "        dims=None,\n",
    "        drop_path_rate=0.0,\n",
    "        layer_scale_init_value=1e-6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dims is None:\n",
    "            dims = [96, 192, 384, 768]\n",
    "        if depths is None:\n",
    "            depths = [3, 3, 9, 3]\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=7),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 52 * 52, 32), nn.ReLU(True), nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        self.downsample_layers = (\n",
    "            nn.ModuleList()\n",
    "        )  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n",
    "                SELayer(dims[i + 1]),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = (\n",
    "            nn.ModuleList()\n",
    "        )  # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[\n",
    "                    Block(\n",
    "                        dim=dims[i],\n",
    "                        drop_path=dp_rates[cur + j],\n",
    "                        layer_scale_init_value=layer_scale_init_value,\n",
    "                    )\n",
    "                    for j in range(depths[i])\n",
    "                ]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
    "        self.attention = DotProductSelfAttention(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(\n",
    "            torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 52 * 52)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=True)\n",
    "        x = F.grid_sample(x, grid, align_corners=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(\n",
    "            x.mean([-2, -1])\n",
    "        )  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.stn(x)\n",
    "        x = self.forward_features(x)\n",
    "        _, weights = self.attention(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            mean_attention_weight = torch.mean(weights)\n",
    "            attention_loss = torch.mean((weights - mean_attention_weight) ** 2)\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels, label_smoothing=0.2) + attention_loss\n",
    "            return torch.argmax(logits, dim=1), logits, loss\n",
    "\n",
    "        return torch.argmax(logits, dim=1), logits\n",
    "\n",
    "\n",
    "def get_model(num_classes, model_size=\"tiny\", in_22k=False):\n",
    "    if model_size == \"tiny\":\n",
    "        depths = [3, 3, 9, 3]\n",
    "        dims = [96, 192, 384, 768]\n",
    "        url = (\n",
    "            model_urls[\"convnext_tiny_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_tiny_1k\"]\n",
    "        )\n",
    "    elif model_size == \"small\":\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [96, 192, 384, 768]\n",
    "        url = (\n",
    "            model_urls[\"convnext_small_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_small_1k\"]\n",
    "        )\n",
    "    elif model_size == \"base\":\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [128, 256, 512, 1024]\n",
    "        url = (\n",
    "            model_urls[\"convnext_base_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_base_1k\"]\n",
    "        )\n",
    "    elif model_size == \"large\":\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [192, 384, 768, 1536]\n",
    "        url = (\n",
    "            model_urls[\"convnext_large_22k\"]\n",
    "            if in_22k\n",
    "            else model_urls[\"convnext_large_1k\"]\n",
    "        )\n",
    "    else:\n",
    "        depths = [3, 3, 27, 3]\n",
    "        dims = [256, 512, 1024, 2048]\n",
    "        url = model_urls[\"convnext_xlarge_22k\"]\n",
    "\n",
    "    default_num_classes = 1000\n",
    "    if in_22k:\n",
    "        default_num_classes = 21841\n",
    "\n",
    "    net = EmoNeXt(\n",
    "        depths=depths, dims=dims, num_classes=default_num_classes, drop_path_rate=0.1\n",
    "    )\n",
    "\n",
    "    state_dict = load_state_dict_from_url(url=url)\n",
    "    net.load_state_dict(state_dict[\"model\"], strict=False)\n",
    "    net.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:40:10.976026Z",
     "iopub.status.busy": "2025-02-18T01:40:10.975559Z",
     "iopub.status.idle": "2025-02-18T01:40:10.984852Z",
     "shell.execute_reply": "2025-02-18T01:40:10.983018Z",
     "shell.execute_reply.started": "2025-02-18T01:40:10.975982Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CosineAnnealingWithWarmRestartsLR(LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        warmup_steps: int = 128,\n",
    "        cycle_steps: int = 512,\n",
    "        min_lr: float = 0.0,\n",
    "        max_lr: float = 1e-3,\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.cycle_steps = cycle_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "        self.steps_counter = 0\n",
    "\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        self.steps_counter += 1\n",
    "\n",
    "        current_cycle_steps = self.steps_counter % self.cycle_steps\n",
    "\n",
    "        if current_cycle_steps < self.warmup_steps:\n",
    "            current_lr = (\n",
    "                self.min_lr\n",
    "                + (self.max_lr - self.min_lr) * current_cycle_steps / self.warmup_steps\n",
    "            )\n",
    "        else:\n",
    "            current_lr = (\n",
    "                self.min_lr\n",
    "                + (self.max_lr - self.min_lr)\n",
    "                * (\n",
    "                    1\n",
    "                    + math.cos(\n",
    "                        math.pi\n",
    "                        * (current_cycle_steps - self.warmup_steps)\n",
    "                        / (self.cycle_steps - self.warmup_steps)\n",
    "                    )\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = current_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:40:23.980197Z",
     "iopub.status.busy": "2025-02-18T01:40:23.979747Z",
     "iopub.status.idle": "2025-02-18T01:40:24.013931Z",
     "shell.execute_reply": "2025-02-18T01:40:24.012686Z",
     "shell.execute_reply.started": "2025-02-18T01:40:23.980164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        training_dataloader,\n",
    "        validation_dataloader,\n",
    "        testing_dataloader,\n",
    "        classes,\n",
    "        output_dir,\n",
    "        max_epochs: int = 10000,\n",
    "        early_stopping_patience: int = 12,\n",
    "        execution_name=None,\n",
    "        lr: float = 1e-4,\n",
    "        amp: bool = False,\n",
    "        ema_decay: float = 0.99,\n",
    "        ema_update_every: int = 16,\n",
    "        gradient_accumulation_steps: int = 1,\n",
    "        checkpoint_path: str = None,\n",
    "    ):\n",
    "        self.epochs = max_epochs\n",
    "\n",
    "        self.training_dataloader = training_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        self.testing_dataloader = testing_dataloader\n",
    "\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(classes)\n",
    "\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        print(\"Device used: \" + self.device.type)\n",
    "\n",
    "        self.amp = amp\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "\n",
    "        self.model = model\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        self.scaler = torch.amp.GradScaler(device=self.device, enabled=self.amp)\n",
    "        self.scheduler = CosineAnnealingWithWarmRestartsLR(\n",
    "            self.optimizer, warmup_steps=128, cycle_steps=1024, max_lr=1e-5\n",
    "        )\n",
    "        self.ema = EMA(model, beta=ema_decay, update_every=ema_update_every).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "\n",
    "        self.output_directory = Path(output_dir)\n",
    "        self.output_directory.mkdir(exist_ok=True)\n",
    "\n",
    "        self.best_val_accuracy = 0\n",
    "\n",
    "        self.execution_name = \"model\" if execution_name is None else execution_name\n",
    "\n",
    "        if checkpoint_path:\n",
    "            self.load(checkpoint_path)\n",
    "\n",
    "        wandb.watch(model, log=\"all\")\n",
    "\n",
    "    def run(self):\n",
    "        counter = 0  # Counter for epochs with no validation loss improvement\n",
    "\n",
    "        images, _ = next(iter(self.training_dataloader))\n",
    "        images = [transforms.ToPILImage()(image) for image in images]\n",
    "        wandb.log({\"Images\": [wandb.Image(image) for image in images]})\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"[Epoch: %d/%d]\" % (epoch + 1, self.epochs))\n",
    "\n",
    "            self.visualize_stn()\n",
    "            train_loss, train_accuracy = self.train_epoch()\n",
    "            val_loss, val_accuracy = self.val_epoch()\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"Train Loss\": train_loss,\n",
    "                    \"Val Loss\": val_loss,\n",
    "                    \"Train Accuracy\": train_accuracy,\n",
    "                    \"Val Accuracy\": val_accuracy,\n",
    "                    \"Epoch\": epoch + 1,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_accuracy > self.best_val_accuracy:\n",
    "                self.save()\n",
    "                counter = 0\n",
    "                self.best_val_accuracy = val_accuracy\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= self.early_stopping_patience:\n",
    "                    print(\n",
    "                        \"Validation loss did not improve for %d epochs. Stopping training.\"\n",
    "                        % self.early_stopping_patience\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        self.test_model()\n",
    "        wandb.finish()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        avg_accuracy = []\n",
    "        avg_loss = []\n",
    "\n",
    "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.training_dataloader))\n",
    "        for batch_idx, data in enumerate(self.training_dataloader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type, enabled=self.amp):\n",
    "                predictions, _, loss = self.model(inputs, labels)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                self.scaler.update()\n",
    "                self.ema.update()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            batch_accuracy = (predictions == labels).sum().item() / labels.size(0)\n",
    "\n",
    "            avg_loss.append(loss.item())\n",
    "            avg_accuracy.append(batch_accuracy)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\"loss\": np.mean(avg_loss), \"acc\": np.mean(avg_accuracy) * 100.0}\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return np.mean(avg_loss), np.mean(avg_accuracy) * 100.0\n",
    "\n",
    "    def val_epoch(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        avg_loss = []\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        pbar = tqdm(\n",
    "            unit=\"batch\", file=sys.stdout, total=len(self.validation_dataloader)\n",
    "        )\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.validation_dataloader):\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type, enabled=self.amp):\n",
    "                predictions, _, loss = self.model(inputs, labels)\n",
    "\n",
    "            avg_loss.append(loss.item())\n",
    "            predicted_labels.extend(predictions.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        accuracy = (\n",
    "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=true_labels,\n",
    "                    preds=predicted_labels,\n",
    "                    class_names=self.classes,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Eval loss: %.4f, Eval Accuracy: %.4f %%\"\n",
    "            % (np.mean(avg_loss) * 1.0, accuracy * 100.0)\n",
    "        )\n",
    "        return np.mean(avg_loss), accuracy * 100.0\n",
    "\n",
    "    def test_model(self):\n",
    "        self.ema.eval()\n",
    "\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "\n",
    "        pbar = tqdm(unit=\"batch\", file=sys.stdout, total=len(self.testing_dataloader))\n",
    "        for batch_idx, (inputs, labels) in enumerate(self.testing_dataloader):\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            with torch.autocast(self.device.type, enabled=self.amp):\n",
    "                _, logits = self.ema(inputs)\n",
    "            outputs_avg = logits.view(bs, ncrops, -1).mean(1)\n",
    "            predictions = torch.argmax(outputs_avg, dim=1)\n",
    "\n",
    "            predicted_labels.extend(predictions.tolist())\n",
    "            true_labels.extend(labels.tolist())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        accuracy = (\n",
    "            torch.eq(torch.tensor(predicted_labels), torch.tensor(true_labels))\n",
    "            .float()\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        print(\"Test Accuracy: %.4f %%\" % (accuracy * 100.0))\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=true_labels,\n",
    "                    preds=predicted_labels,\n",
    "                    class_names=self.classes,\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def visualize_stn(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        batch = torch.utils.data.Subset(val_dataset, range(32))\n",
    "\n",
    "        # Access the batch data\n",
    "        batch = torch.stack([batch[i][0] for i in range(len(batch))]).to(self.device)\n",
    "        with torch.autocast(self.device.type, enabled=self.amp):\n",
    "            stn_batch = self.model.stn(batch)\n",
    "\n",
    "        to_pil = transforms.ToPILImage()\n",
    "\n",
    "        grid = to_pil(torchvision.utils.make_grid(batch, nrow=16, padding=4))\n",
    "        stn_batch = to_pil(torchvision.utils.make_grid(stn_batch, nrow=16, padding=4))\n",
    "\n",
    "        wandb.log({\"batch\": wandb.Image(grid), \"stn\": wandb.Image(stn_batch)})\n",
    "\n",
    "    def save(self):\n",
    "        data = {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"opt\": self.optimizer.state_dict(),\n",
    "            \"ema\": self.ema.state_dict(),\n",
    "            \"scaler\": self.scaler.state_dict(),\n",
    "            \"scheduler\": self.scheduler.state_dict(),\n",
    "            \"best_acc\": self.best_val_accuracy,\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.output_directory / f\"{self.execution_name}.pt\"))\n",
    "        # artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "        # artifact.add_file(\"model.pth\")\n",
    "        # wandb.log_artifact(artifact)\n",
    "\n",
    "    def load(self, path):\n",
    "        data = torch.load(path, map_location=self.device)\n",
    "\n",
    "        self.model.load_state_dict(data[\"model\"])\n",
    "        self.optimizer.load_state_dict(data[\"opt\"])\n",
    "        self.ema.load_state_dict(data[\"ema\"])\n",
    "        self.scaler.load_state_dict(data[\"scaler\"])\n",
    "        self.scheduler.load_state_dict(data[\"scheduler\"])\n",
    "        self.best_val_accuracy = data[\"best_acc\"]\n",
    "\n",
    "\n",
    "def plot_images():\n",
    "    # Create a grid of images for visualization\n",
    "    num_rows = 4\n",
    "    num_cols = 8\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 5))\n",
    "\n",
    "    # Plot the images\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            index = i * num_cols + j  # Calculate the corresponding index in the dataset\n",
    "            image, _ = train_dataset[index]  # Get the image\n",
    "            axes[i, j].imshow(\n",
    "                image.permute(1, 2, 0)\n",
    "            )  # Convert tensor to PIL image format and plot\n",
    "            axes[i, j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"images.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def repeat_tensor(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "\n",
    "def repeat_crops(crops):\n",
    "    return torch.stack([crop.repeat(3, 1, 1) for crop in crops])\n",
    "\n",
    "\n",
    "def crop(crops):\n",
    "    return torch.stack([transforms.ToTensor()(crop) for crop in crops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T01:45:37.714313Z",
     "iopub.status.busy": "2025-02-18T01:45:37.713853Z",
     "iopub.status.idle": "2025-02-18T01:46:31.733261Z",
     "shell.execute_reply": "2025-02-18T01:46:31.731185Z",
     "shell.execute_reply.started": "2025-02-18T01:45:37.714280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 55072 images for training.\n",
      "Using 600 images for evaluation.\n",
      "Using 8333 images for testing.\n",
      "['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "opt = Namespace(\n",
    "    dataset_path=\"/Users/fabalcu97/Programming/University/GiMeFive-dataset/parsed-dataset\",\n",
    "    output_dir=\"out\",\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    lr=1e-5,\n",
    "    amp=True,\n",
    "    in_22k=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_workers=4,\n",
    "    # Used to load a previous save\n",
    "    # checkpoint='/kaggle/working',\n",
    "    checkpoint=None,\n",
    "    model_size=\"tiny\",\n",
    "    early_stopping_patience=20\n",
    ")\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "exec_name = f\"EmoNeXt_{opt.model_size}_{current_time}\"\n",
    "\n",
    "wandb.init(project=\"EmoNeXt\", name=exec_name, anonymous=\"must\")\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(236),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        repeat_tensor,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(236),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        repeat_tensor,\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(236),\n",
    "        transforms.TenCrop(224),\n",
    "        crop,\n",
    "        repeat_crops,\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.ImageFolder(opt.dataset_path + \"/train\", train_transform)\n",
    "val_dataset = datasets.ImageFolder(opt.dataset_path + \"/valid\", val_transform)\n",
    "test_dataset = datasets.ImageFolder(opt.dataset_path + \"/test\", test_transform)\n",
    "\n",
    "print(\"Using %d images for training.\" % len(train_dataset))\n",
    "print(\"Using %d images for evaluation.\" % len(val_dataset))\n",
    "print(\"Using %d images for testing.\" % len(test_dataset))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.num_workers,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "net = get_model(len(train_dataset.classes), opt.model_size, in_22k=opt.in_22k)\n",
    "\n",
    "print(train_dataset.classes)\n",
    "Trainer(\n",
    "    model=net,\n",
    "    training_dataloader=train_loader,\n",
    "    validation_dataloader=val_loader,\n",
    "    testing_dataloader=test_loader,\n",
    "    classes=train_dataset.classes,\n",
    "    execution_name=exec_name,\n",
    "    lr=opt.lr,\n",
    "    early_stopping_patience=opt.early_stopping_patience,\n",
    "    output_dir=opt.output_dir,\n",
    "    checkpoint_path=opt.checkpoint,\n",
    "    max_epochs=opt.epochs,\n",
    "    amp=opt.amp,\n",
    ").run()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
